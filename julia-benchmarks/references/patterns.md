# Benchmark Patterns

## In-Place Operations

Use `evals=1` and `setup` for functions that modify state:

```julia
SUITE["inplace"]["modify"] = @benchmarkable modify!(arr) setup=(arr=zeros(1000)) evals=1
```

## Interpolation

Use `$` to interpolate variables (avoids global variable overhead):

```julia
const data = generate_data()
SUITE["process"] = @benchmarkable process($data)
```

## Size Scaling

Benchmark across different input sizes:

```julia
for n in [10, 100, 1000, 10000]
    SUITE["scale"]["$n"] = @benchmarkable algorithm(data) setup=(data=generate($n))
end
```

## Nested Groups

```julia
SUITE["algorithms"] = BenchmarkGroup()
SUITE["algorithms"]["v1"] = BenchmarkGroup()
SUITE["algorithms"]["v1"]["100"] = @benchmarkable algorithm_v1(n) setup=(n=100)
SUITE["algorithms"]["v1"]["1000"] = @benchmarkable algorithm_v1(n) setup=(n=1000)
```

## Best Practices

1. **Use StableRNGs** for reproducible random data across runs
2. **Create test data outside benchmarks** using `const` globals
3. **Use `evals=1`** for mutating functions
4. **Use `setup=`** for fresh data each evaluation
5. **Interpolate with `$`** to avoid global variable overhead
6. **Group related benchmarks** for organization
7. **Benchmark multiple sizes** to catch scaling issues

## When to Add Benchmarks

- New compilation passes or transformations
- New algorithms or data structures
- Functions that process data at scale
- Performance-critical code paths
- Code where regressions would be problematic
